{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice: réseaux de neurones en Python\n",
    "\n",
    "Nous allons nous intéresser à un jeu de données décrivant des personnes atteintes ou non de maladies cardiaques, avec plusieurs caractéristiques. Le but va être d'essayer de prédire, selon un jeu d'attributs, si oui ou non la personne risque d'avoir une maladie cardiaque.\n",
    "\n",
    "Le jeu de données est téléchargeable sur Arche, \"Données maladie cardiaque\". Le fichier téléchargé devrait avoir comme nom \"heart.csv\".\n",
    "\n",
    "Pour lire et manipuler un fichier csv sur python, je vous conseille d'utiliser la librairie pandas. La documentation de pandas est accessible via ce lien: https://pandas.pydata.org/docs/getting_started/index.html . Installation: `pip install pandas`\n",
    "\n",
    "Pour vous aider, voici comment faire quelques manipulations de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
      "0     63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
      "1     37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
      "2     41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
      "3     56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
      "4     57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
      "..   ...  ...  ..       ...   ...  ...  ...    ...      ...    ...  ..   ...     ...\n",
      "298   57    0   0       140   241    0  ...      1      0.2      1   0     3       0\n",
      "299   45    1   3       110   264    0  ...      0      1.2      1   0     3       0\n",
      "300   68    1   0       144   193    1  ...      0      3.4      1   2     3       0\n",
      "301   57    1   0       130   131    0  ...      1      1.2      1   1     3       0\n",
      "302   57    0   1       130   236    0  ...      0      0.0      1   1     2       0\n",
      "\n",
      "[303 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"heart.csv\") # This supposes that the file is in the same directory as the code\n",
    "print(data) # Prints the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
      "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
      "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
      "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
      "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
      "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
      "\n",
      "[5 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.head()) # Prints the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n"
     ]
    }
   ],
   "source": [
    "print(list(data.columns)) # List of all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fbs  restecg  thalach  exang  oldpeak\n",
      "0      1        0      150      0      2.3\n",
      "1      0        1      187      0      3.5\n",
      "2      0        0      172      0      1.4\n",
      "3      0        1      178      0      0.8\n",
      "4      0        1      163      1      0.6\n",
      "..   ...      ...      ...    ...      ...\n",
      "298    0        1      123      1      0.2\n",
      "299    0        1      132      0      1.2\n",
      "300    1        1      141      0      3.4\n",
      "301    0        1      115      1      1.2\n",
      "302    0        0      174      0      0.0\n",
      "\n",
      "[303 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "sub_data = data.iloc[:,5:10] # Select columns 5 to 9 (python indices start at 0)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sex  cp  chol\n",
      "0      1   3   233\n",
      "1      1   2   250\n",
      "2      0   1   204\n",
      "3      1   1   236\n",
      "4      0   0   354\n",
      "..   ...  ..   ...\n",
      "298    0   0   241\n",
      "299    1   3   264\n",
      "300    1   0   193\n",
      "301    1   0   131\n",
      "302    0   1   236\n",
      "\n",
      "[303 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "subdata = data[[\"sex\",\"cp\",\"chol\"]] # Select columns from a list of keys\n",
    "print(subdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63, 37, 41, 56, 57, 44, 52, 54, 48, 49, 64, 58, 50, 66, 43, 69, 59,\n",
       "       42, 61, 40, 71, 51, 65, 53, 46, 45, 39, 47, 62, 34, 35, 29, 55, 60,\n",
       "       67, 68, 74, 76, 70, 38, 77])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.age.unique() # Prints all the possible value for the key \"age\". Syntax: data.key.unique() where key is the name of a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe d'autres manipulations plus complexes (par exemple, sélectionner des lignes selon une condition, créer une nouvelle colonne depuis d'autres...), mais pour cet exercice ce n'est pas forcément nécessaire. Les personnes curieuses pourront lire la documentation jointe plus haut.\n",
    "\n",
    "# Exercice\n",
    "Le but de l'exercice est de prédire, selon les attributs de votre choix, la présence ou non d'une maladie cardiaque. C'est à dire prédire la colonne `target` du jeu de données. \n",
    "\n",
    "Voici donc les contraintes:\n",
    "\n",
    "1) Vous devez prédire la colonne `target` du jeu de données, elle ne doit pas être utilisée comme attribut en entrée du modèle.\n",
    "\n",
    "2) Vous devez utiliser un réseau de neurones, vous êtes cependant libre d'utiliser la libraire de votre choix: tensorflow (que je vous ai présenté la dernière fois), ou une autre comme pytorch... A vous de voir avec quoi vous êtes le plus à l'aise. Je risque cependant de moins pouvoir vous aider d'un point de vue technique et implémentation si vous choisissez autre chose que tensorflow.\n",
    "\n",
    "3) Vous devez utiliser 80% des données en entraînement et 20% en test.\n",
    "\n",
    "Vous êtes donc libres de:\n",
    "\n",
    "1) Manipuler les 13 autres colonnes de données comme vous le souhaitez: vous pouvez toutes les utiliser, ou les agréger, ou en enlever...\n",
    "\n",
    "2) Vous pouvez choisir l'architecture de votre choix pour le réseau de neurones, le but étant d'essayer plusieurs jeux de paramètres pour voir ce qui fonctionne le mieux :)\n",
    "\n",
    "Quelques conseils/astuces:\n",
    "\n",
    "1) N'hésitez pas à relancer plusieurs fois un entraînement si nécessaire. Le jeu de données étant petit l'entraînement devrait se faire rapidement, et l'initialisation aléatoire du réseau peut faire varier les résultats d'un essai à l'autre. Idéalement un bon réseau devrait avoir une précision en jeu de test entre 0.8 et 0.86.\n",
    "\n",
    "2) Vous allez peut être tomber sur un modèle qui surapprend. Dans ce cas, il est intéressant de regarder la meilleure précision obtenue au cours de l'entraînement si la précision obtenue à la fin n'est pas maximale.\n",
    "\n",
    "3) Commencez par quelque chose de simple, et complexifiez ensuite petit à petit pour avoir de meilleurs résultats.\n",
    "\n",
    "4) Je vous conseille de passer un peu de temps sur le prétraitement des données. Plus précisément, remarquez que les valeurs d'une colonne à l'autre varient énormément: le sexe est un entier entre 0 et 1, et les valeurs de cholestérol varient entre 100 et 400 environ. Il est courant en prétraitement des données de \"normaliser\" les valeurs des données en entrée pour qu'elles soient toutes au même ordre de grandeur. scikit-learn possède un outil pour automatiquement normaliser les données, StandardScaler: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Exemple ci dessous. \n",
    "\n",
    "**Attention**, si vous choisissez de normaliser vos données, vous devez absolument le faire séparement sur vos données d'entraînement et de test !! C'est à dire utiliser deux scaler différents pour train et test. Autrement, si vous utilisez un seul scaler pour TOUT le jeu de données, vous allez injecter des informations du jeu de test dans le jeu d'entraînement et vice versa..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values for cp:\n",
      "[3 2 1 0]\n",
      "Possible values for chol:\n",
      "[233 250 204 236 354 192 294 263 199 168 239 275 266 211 283 219 340 226\n",
      " 247 234 243 302 212 175 417 197 198 177 273 213 304 232 269 360 308 245\n",
      " 208 264 321 325 235 257 216 256 231 141 252 201 222 260 182 303 265 309\n",
      " 186 203 183 220 209 258 227 261 221 205 240 318 298 564 277 214 248 255\n",
      " 207 223 288 160 394 315 246 244 270 195 196 254 126 313 262 215 193 271\n",
      " 268 267 210 295 306 178 242 180 228 149 278 253 342 157 286 229 284 224\n",
      " 206 167 230 335 276 353 225 330 290 172 305 188 282 185 326 274 164 307\n",
      " 249 341 407 217 174 281 289 322 299 300 293 184 409 259 200 327 237 218\n",
      " 319 166 311 169 187 176 241 131]\n",
      "The values for chol and cp have a completely different scale...\n",
      "Before standardization:\n",
      "     cp  chol\n",
      "0     3   233\n",
      "1     2   250\n",
      "2     1   204\n",
      "3     1   236\n",
      "4     0   354\n",
      "..   ..   ...\n",
      "298   0   241\n",
      "299   3   264\n",
      "300   0   193\n",
      "301   0   131\n",
      "302   1   236\n",
      "\n",
      "[303 rows x 2 columns]\n",
      "After standardization (first 10 rows only):\n",
      "[[ 1.97312292 -0.25633371]\n",
      " [ 1.00257707  0.07219949]\n",
      " [ 0.03203122 -0.81677269]\n",
      " [ 0.03203122 -0.19835726]\n",
      " [-0.93851463  2.08204965]\n",
      " [-0.93851463 -1.04867848]\n",
      " [ 0.03203122  0.92252071]\n",
      " [ 0.03203122  0.32343076]\n",
      " [ 1.00257707 -0.91340011]\n",
      " [ 1.00257707 -1.51249006]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "subdata = data[[\"cp\",\"chol\"]]\n",
    "\n",
    "print(\"Possible values for cp:\")\n",
    "print(subdata.cp.unique())\n",
    "print(\"Possible values for chol:\")\n",
    "print(subdata.chol.unique())\n",
    "print(\"The values for chol and cp have a completely different scale...\")\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(subdata)\n",
    "\n",
    "scaler = preprocessing.StandardScaler() # We create the scaler\n",
    "standardized_subdata = scaler.fit_transform(subdata) # We use the scaler to \"fit\" the normal distribution on the data and then transform it.\n",
    "\n",
    "print(\"After standardization (first 10 rows only):\")\n",
    "print(standardized_subdata[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
